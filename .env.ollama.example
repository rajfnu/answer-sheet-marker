# ============================================================
# Answer Sheet Marker - Ollama Local LLM Configuration
# ============================================================
# This configuration uses Ollama for local, free LLMs
# Best for: Privacy, no API costs, offline use
# Cost: $0 (Free! - hardware costs only)

# LLM Provider Settings
LLM_PROVIDER=ollama
LLM_MODEL=llama3:70b  # Options: llama3:70b, llama3, mistral, phi3:medium
LLM_BASE_URL=http://localhost:11434

# Note: Make sure Ollama is running:
#   1. Start Ollama: ollama serve
#   2. Download model: ollama pull llama3:70b
#   3. Check status: ollama list

# LLM Parameters
MAX_TOKENS=8192
TEMPERATURE=0.0

# Processing Configuration (may need to reduce for local models)
BATCH_SIZE=3  # Reduced for local processing
MAX_CONCURRENT_REQUESTS=1  # Process one at a time locally

# Quality Thresholds
MIN_CONFIDENCE_SCORE=0.7
REQUIRE_HUMAN_REVIEW_BELOW=0.6

# Storage
VECTOR_DB_PATH=./data/vector_db
CACHE_ENABLED=true

# Logging
LOG_LEVEL=INFO
LOG_FILE=./logs/app.log

# Document Processing
PDF_DPI=300
OCR_LANGUAGE=eng
OCR_CONFIG=--psm 6

# Output
OUTPUT_DIRECTORY=./output
